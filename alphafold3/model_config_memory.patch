--- src/alphafold3/model/model_config.py.org	2025-02-06 09:48:18.245611934 -0600
+++ src/alphafold3/model/model_config.py	2025-02-06 16:37:16.232549571 -0600
@@ -8,6 +8,8 @@
 # if received directly from Google. Use is subject to terms of use available at
 # https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md
 
+import os
+
 """Global config for the model."""
 
 from collections.abc import Sequence
@@ -16,6 +18,10 @@
 from alphafold3.common import base_config
 from alphafold3.jax.attention import attention
 
+try:
+    gpu_mem_mb = int(os.environ["CUDA_MEMORY_AVAIL"])
+except:
+    gpu_mem_mb = float('inf')
 
 _Shape2DType: TypeAlias = tuple[int | None, int | None]
 
@@ -24,9 +30,22 @@
   bfloat16: Literal['all', 'none', 'intermediate'] = 'all'
   final_init: Literal['zeros', 'linear'] = 'zeros'
   pair_attention_chunk_size: Sequence[_Shape2DType] = ((1536, 128), (None, 32))
-  pair_transition_shard_spec: Sequence[_Shape2DType] = (
-      (2048, None),
-      (None, 1024),
-  )
+
+  if gpu_mem_mb < 50000:
+      # 40GBs parameters
+      print('Adjusting pair_transition_shard_spec for GPUs with less than 50GBs of memory...', flush=True)
+      pair_transition_shard_spec: Sequence[_Shape2DType] = (
+          (2048, None),
+          (3072, 1024),
+          (None, 521),
+      )
+  else:
+      # 80GBs parameters
+      print('Adjusting pair_transition_shard_spec for GPUs with more than 50GBs of memory...', flush=True)
+      pair_transition_shard_spec: Sequence[_Shape2DType] = (
+          (2048, None),
+          (None, 1024),
+      )
+
   # Note: flash_attention_implementation = 'xla' means no flash attention.
   flash_attention_implementation: attention.Implementation = 'triton'
